{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Fetching URLs for data scraping and saving it into 5 different files.\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Coins to scrape\n",
    "COINS = [\"dogecoin\", \"bitcoin\", \"ethereum\", \"solana\", \"hamster\"]\n",
    "\n",
    "# Base URL pattern\n",
    "BASE_URL = \"https://thenewscrypto.com/page/{}/?s={}\"  # Format: (page_num, coin_name)\n",
    "\n",
    "# User-Agent to avoid getting blocked\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def get_article_links(coin, page_num):\n",
    "    \"\"\"Fetches up to 10 article links from a specific coin search results page.\"\"\"\n",
    "    url = BASE_URL.format(page_num, coin)\n",
    "    logging.info(f\"Scraping {coin} - Page {page_num}: {url}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            logging.warning(f\"Failed to fetch {coin} page {page_num}, status code: {response.status_code}\")\n",
    "            return []\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        articles = soup.find_all(\"h3\", class_=\"card-title fs-17\")\n",
    "        \n",
    "        # Extract up to 10 article links\n",
    "        links = []\n",
    "        for article in articles[:10]:\n",
    "            link_tag = article.find(\"a\")\n",
    "            if link_tag and \"href\" in link_tag.attrs:\n",
    "                links.append(link_tag[\"href\"])\n",
    "\n",
    "        logging.info(f\"Found {len(links)} articles for {coin} on page {page_num}\")\n",
    "        return links\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error fetching {coin} page {page_num}: {e}\")\n",
    "        return []\n",
    "\n",
    "def scrape_articles_for_coin(coin):\n",
    "    \"\"\"Scrapes up to 10 article links for a specific coin and saves them to a CSV file.\"\"\"\n",
    "    links = get_article_links(coin, 1)  # Fetch only the first page\n",
    "    \n",
    "    if links:\n",
    "        output_csv = f\"{coin}_news_urls.csv\"\n",
    "        with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\"Coin\", \"Article URL\"])  # CSV header\n",
    "            for link in links:\n",
    "                writer.writerow([coin, link])\n",
    "        \n",
    "        logging.info(f\"Saved {len(links)} articles for {coin} to {output_csv}\")\n",
    "    else:\n",
    "        logging.warning(f\"No articles found for {coin}.\")\n",
    "\n",
    "# Run the scraper for each coin\n",
    "if __name__ == \"__main__\":\n",
    "    for coin in COINS:\n",
    "        scrape_articles_for_coin(coin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Extracting the data from the web URLs\n",
    "import trafilatura\n",
    "import csv\n",
    "import pdfkit  #to use this we need to install wkhtmltopdf from https://wkhtmltopdf.org/downloads.html\n",
    "\n",
    "# Coins to process\n",
    "COINS = [\"dogecoin\", \"bitcoin\", \"ethereum\", \"solana\", \"hamster\"]\n",
    "\n",
    "def process_coin(coin):\n",
    "    csv_file = f\"{coin}_news_urls.csv\"\n",
    "    urls = []\n",
    "\n",
    "    # Read URLs from CSV\n",
    "    try:\n",
    "        with open(csv_file, newline='', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            next(reader)  # Skip header\n",
    "            for row in reader:\n",
    "                if row:\n",
    "                    clean_url = row[1].strip().strip(\"'\")\n",
    "                    urls.append(clean_url)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ CSV file not found for {coin}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loaded URLs for {coin}:\", urls)\n",
    "\n",
    "    articles_content = \"\"\n",
    "\n",
    "    for url in urls:\n",
    "        print(f\"Fetching: {url}\")\n",
    "        try:\n",
    "            downloaded = trafilatura.fetch_url(url)\n",
    "            if downloaded:\n",
    "                content = trafilatura.extract(downloaded)\n",
    "                if content:\n",
    "                    print(f\"✅ Extracted content from {url}\")\n",
    "                    articles_content += f\"<p>{content}</p>\\n\"\n",
    "                else:\n",
    "                    print(f\"❌ No content extracted from {url}\")\n",
    "            else:\n",
    "                print(f\"❌ Failed to download {url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error fetching {url}: {e}\")\n",
    "\n",
    "    # Save extracted content to an HTML file\n",
    "    if articles_content:\n",
    "        html_file = f\"{coin}_articles.html\"\n",
    "        pdf_file = f\"{coin}_data.pdf\"\n",
    "        \n",
    "        with open(html_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(articles_content)\n",
    "        print(f\"✅ {html_file} successfully created!\")\n",
    "        \n",
    "        pdfkit.from_file(html_file, pdf_file)\n",
    "        print(f\"✅ PDF successfully generated: {pdf_file}\")\n",
    "    else:\n",
    "        print(f\"❌ No articles extracted for {coin}, check URLs or website restrictions.\")\n",
    "\n",
    "# Run the process for each coin\n",
    "if __name__ == \"__main__\":\n",
    "    for coin in COINS:\n",
    "        process_coin(coin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Converting the pdf data into txt\n",
    "import pdfplumber\n",
    "\n",
    "def extract_text_fast(pdf_files, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for pdf_path in pdf_files:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for i, page in enumerate(pdf.pages):\n",
    "                    text = page.extract_text()\n",
    "                    if text:  # Only write non-empty text\n",
    "                        f.write(text + \"\\n\")\n",
    "\n",
    "                    # Print progress every 100 pages\n",
    "                    if (i + 1) % 100 == 0:\n",
    "                        print(f\"Processed {i + 1}/{len(pdf.pages)} pages from {pdf_path}...\")\n",
    "\n",
    "# List of PDF files generated from the previous script\n",
    "pdf_files = [\"dogecoin_data.pdf\", \"bitcoin_data.pdf\", \"ethereum_data.pdf\", \"solana_data.pdf\", \"hamster_data.pdf\"]\n",
    "output_file = \"output.txt\"\n",
    "extract_text_fast(pdf_files, output_file)\n",
    "\n",
    "print(\"Extraction completed! Check output.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
