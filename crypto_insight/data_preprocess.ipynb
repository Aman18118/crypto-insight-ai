{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pip install tqdm unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lucku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lucku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lucku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences to process: 102816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:  10%|▉         | 10238/102816 [00:05<00:41, 2253.09sent/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000/102816 sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:  20%|█▉        | 20407/102816 [00:10<00:36, 2244.82sent/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20000/102816 sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:  29%|██▉       | 30262/102816 [00:14<00:30, 2407.94sent/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30000/102816 sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:  39%|███▉      | 40426/102816 [00:18<00:25, 2481.18sent/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 40000/102816 sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:  49%|████▉     | 50487/102816 [00:22<00:21, 2416.42sent/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50000/102816 sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:  59%|█████▊    | 60237/102816 [00:26<00:19, 2185.04sent/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 60000/102816 sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:  69%|██████▊   | 70444/102816 [00:31<00:13, 2466.99sent/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 70000/102816 sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:  78%|███████▊  | 80437/102816 [00:35<00:10, 2183.76sent/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 80000/102816 sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:  88%|████████▊ | 90372/102816 [00:40<00:05, 2119.56sent/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 90000/102816 sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:  98%|█████████▊| 100290/102816 [00:45<00:01, 2114.29sent/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100000/102816 sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|██████████| 102816/102816 [00:46<00:00, 2201.09sent/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaning complete! File saved as cleaned_data.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import unidecode\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Ensure NLTK resources are available\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Initialize lemmatizer and stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# List of crypto-related terms to keep intact\n",
    "crypto_terms = {\"bitcoin\", \"btc\", \"ethereum\", \"eth\", \"blockchain\", \"crypto\", \"nft\", \"defi\", \"web3\", \"dao\", \"altcoin\",\"solana\",\"hamster\",\"dogecoin\"\n",
    "                \"stablecoin\", \"smart contract\", \"staking\", \"mining\", \"wallet\", \"coin\", \"token\", \"airdrop\", \"fomo\", \"hodl\"}\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    \"\"\" Clean, tokenize, stem, and lemmatize a sentence while preserving crypto terms. \"\"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # Remove unwanted special characters (like â€™, emojis, etc.)\n",
    "    sentence = unidecode.unidecode(sentence)  # Normalize special characters\n",
    "    sentence = re.sub(r\"[^\\x00-\\x7F]+\", \" \", sentence)  # Remove non-ASCII characters\n",
    "    sentence = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", sentence)  # Remove punctuations except spaces\n",
    "    \n",
    "    # Tokenize words\n",
    "    words = word_tokenize(sentence)\n",
    "    \n",
    "    cleaned_words = []\n",
    "    for word in words:\n",
    "        if word in crypto_terms:  # Preserve crypto-related words\n",
    "            cleaned_words.append(word)\n",
    "        elif word not in stop_words:\n",
    "            stemmed_word = stemmer.stem(word)  # Apply stemming\n",
    "            lemmatized_word = lemmatizer.lemmatize(stemmed_word)  # Apply lemmatization\n",
    "            cleaned_words.append(lemmatized_word)\n",
    "\n",
    "    return \" \".join(cleaned_words)\n",
    "\n",
    "def preprocess_text(sentences):\n",
    "    \"\"\" Process a list of sentences efficiently and provide real-time updates. \"\"\"\n",
    "    cleaned_sentences = []\n",
    "    \n",
    "    # Process each sentence with tqdm progress bar\n",
    "    for i, sentence in enumerate(tqdm(sentences, desc=\"Processing Sentences\", unit=\"sent\")):\n",
    "        cleaned_sentences.append(preprocess_sentence(sentence))\n",
    "\n",
    "        # Print progress every 10,000 sentences\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(f\"Processed {i+1}/{len(sentences)} sentences...\")\n",
    "\n",
    "    return cleaned_sentences\n",
    "\n",
    "# Load text file\n",
    "with open(\"output.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Split into sentences (faster than using nltk's sent_tokenize)\n",
    "sentences = text.split(\"\\n\")\n",
    "\n",
    "print(f\"Total sentences to process: {len(sentences)}\")\n",
    "\n",
    "# Process text\n",
    "cleaned_sentences = preprocess_text(sentences)\n",
    "\n",
    "# Save cleaned sentences\n",
    "with open(\"cleaned_data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(cleaned_sentences))\n",
    "\n",
    "print(\"✅ Cleaning complete! File saved as cleaned_data.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
